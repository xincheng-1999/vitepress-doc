# 大模型学习路线

这部分是一个「能落地」的大模型学习路线：先建立直觉，再补齐工程实践，最后补齐评测与上线。

## 你将学到什么

- 大模型（LLM）的核心机制：Transformer、注意力、Token、上下文窗口
- Prompt 设计与常见范式（Few-shot / CoT / ReAct / Tool calling）
- RAG（检索增强生成）：切分、向量化、召回、重排、引用
- 微调与对齐：SFT、LoRA、偏好优化（RLHF/DPO 的概念）
- 评测：离线指标 + 人评 + 线上 A/B
- 上线：推理性能、缓存、限流、观测、成本控制

## 整体架构
```
┌─────────────────────────────────────────────────────────────────────┐
│                    研究/开发者                                  │
└────────────────────┬──────────────────────────────────────────────────┘
                     │
                     ▼
┌───────────────────────────────────────────────────────────────────┐
│                      1. 训练阶段                         │
│                                                              │
│  ┌────────────────┬────────────────┬───────────────────────┐  │
│  │    PyTorch     │   TensorFlow   │      JAX          │  │
│  │    (Meta)       │   (Google)    │     (DeepMind)    │  │
│  │                 │                │                     │  │
│  └─────────┬───────┴────────────────┴───────────┬───────┘  │
│            ▼                               ▼            │          │
│  ┌─────────────────────────────────────────────────┐          │
│  │      Transformers (Python) 训练/微调工具     │          │
│  └─────────────────┬───────────────────────────────┘          │
│                    ▼                                │          │
│  ┌───────────────────────────────────────────────┐          │
│  │      训练得到模型文件                      │          │
│  │      (.pth / .bin / .safetensors)       │          │
│  └────────────────────┬─────────────────────────────┘          │
└─────────────────────────┼───────────────────────────────────────────────┘
                      │
                      ▼
┌───────────────────────────────────────────────────────────────────┐
│                      2. 模型转换/导出                     │
│                                                              │
│  ┌─────────────────────────────────────┬────────────────────┐ │
│  │   直接使用 PyTorch 格式       │   导出 ONNX 格式   │ │
│  │   （服务器端推理）              │   （浏览器端推理）   │ │
│  └─────────┬──────────────────────────┴──────┬───────────┘ │
│            ▼                              ▼            │
│  ┌──────────────────────┐   ┌──────────────────────┐  │
│  │  .pt/.pth 文件     │   │  .onnx 文件      │  │
│  └─────────┬──────────────┘   └─────────┬──────────────┘  │
│            ▼                              ▼            │
│       ┌─────────────────────────────────────────┐         │
│       │   上传到 Hugging Face Hub         │         │
│       │   （模型仓库）                    │         │
│       └─────────────────┬───────────────────────┘         │
└─────────────────────────┼───────────────────────────────────────┘
                      │
                      ▼
┌───────────────────────────────────────────────────────────────────┐
│                      3. 部署阶段                              │
│                                                              │
│  ┌─────────────────────────────────────────────────────────────┐    │
│  │                  推理环境选择                       │    │
│  └──────────────┬───────────────────────────────────────┬───┘    │
│                 ▼                               ▼        │    │
│  ┌──────────────────┐                 ┌──────────────────┐    │
│  │   服务器端部署     │                 │  浏览器端部署    │    │
│  │   （生产环境）     │                 │  （客户端/边缘）   │    │
│  └─────────┬──────────┘                 └─────────┬──────────┘    │
│             ▼                                     ▼           │
│  ┌─────────────────────────────┐   ┌───────────────────────────┐   │
│  │   Transformers + PyTorch  │   │  Transformers.js + ONNX  │   │
│  │   （或 vLLM 加速）      │   │   （或 WebGPU 加速）     │   │
│  └─────────┬──────────────────┘   └─────────┬───────────────────┘   │
│             ▼                                 │           │
│    ┌────────────────────────────────────┐           │           │
│    │  可选：vLLM 性能优化层        │           │
│    │  - PagedAttention                 │           │
│    │  - 连续批处理                   │           │
│    │  - 多 GPU 并行                   │           │
│    │  - CUDA Graph 编译               │           │
│    └─────────┬──────────────────────────────┘           │
│               ▼                                │
│  ┌───────────────────────────────────────────────────────┐  │
│  │              推理引擎                  │  │
│  └───────────────────────────────────┬───────────────┘  │
│                                  │              │        │
│                                  ▼              │        │
│         ┌──────────────────────┬───────────────────┐        │
│         │  API 服务端       │  浏览器应用     │        │
│         │  (HTTP/gRPC)     │  (React/Vue)    │        │
│         └──────────┬───────────┘   └─────────┬───────────┘        │
└─────────────────────────┼──────────────────┼──────────────────────────────┘
                      │              │
                      ▼              ▼
┌─────────────────────────────────┐  ┌───────────────────────────┐
│      用户/企业调用           │  │   用户浏览器访问          │
└─────────────────────────────────┘  └───────────────────────────┘
```

## 目录

### 第零阶段：环境准备（建议 15-60 分钟）
- [学习环境与安装（Windows）](/llm-study/environment)

### 第一阶段：基础概念（建议 1-3 天）
- [什么是大模型：从 Token 到上下文窗口](/llm-study/intro)
- [Transformer 速通：注意力、位置编码与推理](/llm-study/transformer)

### 第二阶段：Prompt 工程（建议 3-7 天）
- [Prompt 工程：从“能用”到“稳定”](/llm-study/prompting)

### 第三阶段：RAG（建议 1-2 周）
- [RAG 实战要点：切分、召回、重排、引用](/llm-study/rag)

> 建议先跑通 RAG 章节里的“最小 Demo”，再继续做优化。

### 第四阶段：微调与对齐（建议 1-2 周）
- [微调与对齐：SFT/LoRA/DPO 的工程视角](/llm-study/fine-tuning)

> RTX 3050 建议优先：小模型（0.5B~1.5B）+ LoRA/QLoRA，先跑通再追求更大模型。

### 第五阶段：评测与上线（建议 1-2 周）
- [评测：离线、线上与可观测性](/llm-study/evaluation)
- [部署与性能：吞吐、延迟、成本与可靠性](/llm-study/deployment)

> 建议先做 20 条 JSONL 离线评测集 + 一键 runner，再谈上线优化。

### 第六阶段：安全与合规（持续）
- [安全：提示注入、数据泄露与防护](/llm-study/safety)

> 上线前建议做一组“提示注入回归用例”，每次改 prompt/检索策略都要跑。

## 推荐学习方式

- **先做一个小 Demo**：一个带 RAG 的问答页面（哪怕只有命令行）。
- **只跟踪 2-3 个指标**：回答是否引用到来源、平均延迟、单位成本。
- **写“失败案例”笔记**：大模型工程最值钱的是失败集合。
