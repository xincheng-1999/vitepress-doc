# 什么是大模型：从 Token 到上下文窗口

## 一句话理解

大模型就是一个“根据上下文预测下一个 Token”的概率模型。它通过海量数据训练出语言与知识的统计结构，然后在推理阶段按概率一步步生成输出。

这一句话非常重要，因为它解释了 3 个现象：

- **为什么会胡编**：它在“补全”，不是在“查证”。
- **为什么 Prompt 能左右结果**：你给的上下文就是它的全部世界。
- **为什么参数（temperature/top_p）会改变输出稳定性**：它们本质上在改变“随机性”。

## 关键名词

- **Token**：模型处理的最小文本单元（不等于字符/单词）。
- **上下文窗口（Context Window）**：一次推理能“看见”的 Token 数上限。
- **Prompt**：输入给模型的指令与上下文。
- **Completion**：模型生成的输出。

补充几个你后面一定会遇到的名词：

- **System / Developer / User**：对话角色（不同平台叫法略不同），用于区分“规则”和“用户输入”。
- **Embedding（向量）**：把文本映射到向量空间，用于相似度检索（RAG 的基础）。
- **Hallucination（幻觉）**：模型输出了看似合理、但事实不成立的内容。

## 为什么它看起来“会思考”

- 训练数据中包含了大量“问题—解答—推理痕迹”的文本模式
- Transformer 的注意力机制允许模型把“相关的信息”聚合到当前预测中
- 但它本质上仍是概率生成：**会犯错、会编造、会被诱导**

你可以把它当成“非常强的写作/归纳器”，但不是“可信数据库”。工程上，可信通常来自：

- 你给的**资料**（RAG/工具查询）
- 你做的**约束**（结构化输出、校验、拒答策略）
- 你做的**观测与回放**（能定位问题、能迭代）

## 常见误区

- “模型记住了所有知识”：更准确是它学到了统计关联；知识随时间会过期。
- “模型输出越长越好”：长输出更容易漂移，工程上要更强调**结构化**与**可验证**。

再补 2 个新手常踩坑：

- “只要把问题说清楚就行”：现实里输入噪声很大，要靠**模板**与**格式约束**。
- “模型很强，所以不用做评测”：模型越强，越容易让你忽略错误；上线必须做评测与监控。

---

## 1. Token：为什么它不是“字数”

### 1.1 Token 是怎么来的

模型不会直接读“字符”，而是用分词器（tokenizer）把文本切成 Token。

- 英文：通常接近“词的一部分”
- 中文：可能接近“单字/词片段”的混合（不同模型差异很大）

所以同样 100 个汉字，在不同模型上 Token 数可能不同。

### 1.2 为什么你要关心 Token

- **费用**：云端 API 通常按输入/输出 Token 计费
- **延迟**：输出越长越慢（尤其是首 token 之后的生成）
- **上限**：上下文窗口限制了“最多能塞多少资料”

---

## 2. 上下文窗口：一次推理能装多少东西

上下文窗口 = 输入 Token + 输出 Token 的总预算（不同服务商规则略有差异，但你可以先按这个理解）。

当你把对话历史、资料、指令都塞进去时，会遇到：

- **截断**：太长会被裁掉（通常裁最早的内容）
- **遗忘**：即使没被裁掉，长上下文下模型也可能抓不住重点

工程上的 3 个常见策略：

- **摘要历史对话**：保留关键信息，减少 token
- **RAG**：只把“最相关”的片段塞入上下文
- **结构化上下文**：用标题/编号/引用，让模型更容易定位

---

## 3. 采样参数：temperature / top_p / max_tokens

这部分决定“输出更稳定还是更发散”。你不需要背公式，但要知道怎么用。

### 3.1 temperature（温度）

- 越低：越保守、更像“标准答案”（更稳定）
- 越高：越发散、更有创造力（更不稳定）

经验值：

- **问答/抽取/总结**：`temperature = 0 ~ 0.3`
- **头脑风暴/文案**：`temperature = 0.7 ~ 1.0`

### 3.2 top_p（核采样）

控制“候选词的概率覆盖范围”。常见做法是：

- 固定 `top_p = 0.8 ~ 0.95`
- 主要用 temperature 控制风格

### 3.3 max_tokens（最大输出）

工程上非常重要：

- 防止输出过长拖慢/烧钱
- 让接口稳定（避免输出无限扩展）

---

## 4. 立刻动手：两个 5 分钟小实验

你现在已经装好了环境（上一节）。这两个实验的目的：**感受参数对输出的影响**。

### 实验 A：同一问题，不同 temperature

在你上一节的 `hello_llm.py` 基础上，把请求改成这样（只看核心字段即可）：

```python
resp = client.chat.completions.create(
  model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
  temperature=0.0,
  messages=[
    {"role": "system", "content": "你是一个严谨的助教。"},
    {"role": "user", "content": "用 3 句话解释注意力机制。"},
  ],
)
```

然后把 `temperature` 分别改成 `0.0 / 0.7` 运行两次，对比：

- 用词是否更发散
- 是否更容易跑题
- 是否更“自信地胡说”

### 实验 B：本地 Ollama（感受“模型大小/速度”）

如果你走了 Ollama 路线，建议再试一次：

```bash
ollama run llama3.1
```

你会明显感受到：本地模型在“知识新鲜度/表达能力”上可能不如云端，但它更适合你练习工程流程（提示词、结构化输出、RAG）。

---

## 5. 你的显卡：RTX 3050 的现实上限（后面会一直按这个来）

RTX 3050 常见显存有 4GB / 6GB / 8GB（不同笔记本/台式版本差别很大）。在不做训练、只做推理的前提下，可以按下面的保守经验理解：

- **优先目标：7B 级模型 + 4-bit 量化**（更容易跑起来）
- **13B 级模型**：通常会吃紧（看显存、量化方案、上下文长度），不作为默认目标
- **训练/全量微调**：不建议在 3050 上作为主战场；我们后面会以 **LoRA 小样本**作为了解

为了不踩坑，后续教程里我会遵循：

- 以“能跑通”为第一目标（小模型/量化/短上下文）
- 提供云端替代方案（没有 GPU 也能继续学）

---

## 工程落地建议

- 任何面向用户的回答都尽量做到：
  - **可引用**（给出来源/证据）
  - **可约束**（输出格式、字段校验）
  - **可观测**（记录 prompt、检索结果、模型版本、耗时）

如果你只记住一句话：

> 让模型“生成”，让系统“负责正确”。
