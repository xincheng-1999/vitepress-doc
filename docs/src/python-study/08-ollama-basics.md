# 08. Ollama 入门（CPU 也能跑）

本章目标：在**没有独显**的 Windows 机器上，稳定使用 Ollama 进行本地推理。

> 原理补充：你现在的约束是“CPU + 核显”。这意味着：
> - 先追求“能稳定跑通 + 能接入脚本”，再追求质量
> - 更偏向小模型（3B/7B）
> - 需要更强的“输入控制”（不要一次喂超长文本）

## 1) 验证 Ollama 是否可用
```bash
ollama --version
ollama list
```

也可以确认服务是否在本机监听（默认 `11434`）：
```bash
curl http://localhost:11434/api/tags
```

> 原理补充：Ollama 除了 CLI，还提供 HTTP API。后面我们会直接用 HTTP，让脚本可控、可复现。

## 2) 拉取一个轻量模型（示例）
> 你可以按自己已安装的模型来，本章以“能跑通”为第一目标。

```bash
ollama pull llama3.2:3b
```

如果你不确定有哪些模型可选，可以先用自己已经下载的（`ollama list` 里有）。

## 3) 命令行对话
```bash
ollama run llama3.2:3b
```

常用操作：
```bash
# 查看当前运行中的模型
ollama ps

# 停止某个模型（如果它在后台常驻）
ollama stop llama3.2:3b
```

## 4) CPU 场景的现实建议
- 优先小模型（3B/7B）
- 控制上下文长度（长文档要切分）
- 追求稳定优先，不要一次喂太多

额外建议（对“脚本自动化”更重要）：
- 把任务拆成小步：先抽取信息（结构化 JSON），再汇总
- 给模型明确输出格式（后面第 09/10 章会做 JSON 约束）

> 原理补充：CPU 场景下，影响体验的往往不是“模型能不能回答”，而是“你是不是一次给了太多内容 + 要它一次做太多事”。

## 5) 常见问题（快速排查）
- `curl http://localhost:11434/api/tags` 失败：Ollama 服务可能未启动或端口被占用
- `ollama run` 很慢：模型太大/上下文太长/系统资源紧张（先换小模型、缩短输入）
- 输出不稳定：后面用“JSON + 校验失败重试”能显著改善

## 练习
- 用同一个提示词问 3 次：比较 temperature 不同的表现

下一章会用 Python 调用 Ollama，并做“流式输出”。
